{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "671cde50-53ce-4d2e-a709-faff6b8142be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date\n",
    "import glob\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import array, explode, col, from_unixtime, lit, map_from_arrays, make_date, sum, when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "# Point Spark to this exact Python\n",
    "os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## use correct version of python / spark\n",
    "spark = (SparkSession.builder\n",
    "         .master(\"local[*]\")\n",
    "         .appName(\"LocalPySpark\")\n",
    "         .config(\"spark.pyspark.python\", sys.executable)\n",
    "         .config(\"spark.pyspark.driver.python\", sys.executable)\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_root = r\"C:\\Users\\alexa\\OneDrive\\Bureau\\Data Engineering\\Yahoo DataBricks Pipeline\"\n",
    "db_root_python = \"/dbfs\"\n",
    "db_root_spark = \"dbfs:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c63f8b97-689b-418b-8003-a408ffa9edb1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bronze_path = \"/FileStore/bronze/yahoo\"\n",
    "silver_path = \"/FileStore/silver/yahoo\"\n",
    "my_portfolio = ('GOOG', 'SOPH', 'PYPL', 'NOV', 'KRN', 'AMZN', 'NVDA', 'SQN', 'TGT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. call yahoo api & store bronze layer data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8041735c-1c4a-4235-8a3f-5f657271f73b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def unpivot_pandas_statement(ticker, statement):\n",
    "    temp = statement.\\\n",
    "    reset_index().\\\n",
    "    melt(id_vars='index', var_name='date', value_name='value').\\\n",
    "    rename(columns={'index':'metric'})\n",
    "    temp['ticker'] = ticker\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "715a095f-0e5f-42d2-bbc5-7c7b5c8e03c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_raw_yahoo_financial_statements(src_root_path, ticker):\n",
    "    try:\n",
    "        current_ticker = yf.Ticker(ticker)\n",
    "        unpivot_pandas_statement(ticker, current_ticker.get_balance_sheet()).\\\n",
    "            to_json(f'{src_root_path}/{ticker}_balance_sheet_{date.today()}.json', orient='records')\n",
    "        unpivot_pandas_statement(ticker, current_ticker.get_income_stmt())\\\n",
    "            .to_json(f'{src_root_path}/{ticker}_income_stmt_sheet_{date.today()}.json', orient='records')\n",
    "        unpivot_pandas_statement(ticker, current_ticker.get_cashflow()).\\\n",
    "            to_json(f'{src_root_path}/{ticker}_cashflow_{date.today()}.json', orient='records')\n",
    "\n",
    "    except (requests.ConnectionError, requests.Timeout) as e:\n",
    "        print(f\"Connection error for {ticker}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error for {ticker}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7487c16c-c36d-4023-a823-d70653b04a13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"for stock in my_portfolio:\\n    print(f'processing stock {stock}')\\n    get_raw_yahoo_financial_statements(local_root+bronze_path, stock)\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"for stock in my_portfolio:\n",
    "    print(f'processing stock {stock}')\n",
    "    get_raw_yahoo_financial_statements(local_root+bronze_path, stock)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Load income statement + balance sheet + cashflows and UNION from bronze files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_epoch_to_date(df):\n",
    "    return df.withColumn('datetime', F.from_unixtime((F.col(\"date\") / 1000)).cast(T.DateType())).drop(\"date\")\n",
    "def remove_null_values(df):\n",
    "    return df.filter(\"value IS NOT NULL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using glob for files listing mor efficient than spark listing the directory\n",
    "income_stmt_file_names = [file for file in glob.\\\n",
    "                    glob(os.path.\\\n",
    "                    join(f'{local_root}{bronze_path}', \"*income_stmt_sheet*.json\"))]\n",
    "\n",
    "balance_sheet_file_names = [file for file in glob.\\\n",
    "                    glob(os.path.\\\n",
    "                    join(f'{local_root}{bronze_path}', \"*balance_sheet*.json\"))]\n",
    "\n",
    "cashflow_file_names = [file for file in glob.\\\n",
    "                    glob(os.path.\\\n",
    "                    join(f'{local_root}{bronze_path}', \"*cashflow*.json\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of files :\n",
      "sdf_all_income_stmt : 1446\n",
      "sdf_all_balance_sheet : 2695\n",
      "sdf_all_cashflow : 1928\n"
     ]
    }
   ],
   "source": [
    "sdf_all_income_stmt = spark.read.json(income_stmt_file_names)\n",
    "sdf_all_balance_sheet = spark.read.json(balance_sheet_file_names)\n",
    "sdf_all_cashflow = spark.read.json(cashflow_file_names)\n",
    "print(f'length of files :')\n",
    "print(f'sdf_all_income_stmt : {sdf_all_income_stmt.count()}')\n",
    "print(f'sdf_all_balance_sheet : {sdf_all_balance_sheet.count()}')\n",
    "print(f'sdf_all_cashflow : {sdf_all_cashflow.count()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sdf_all_income_stmt : 4820\n"
     ]
    }
   ],
   "source": [
    "sdf_financials = sdf_all_income_stmt.unionAll(sdf_all_balance_sheet).unionAll(sdf_all_cashflow)\n",
    "sdf_financials = remove_null_values(sdf_financials)\n",
    "sdf_financials = convert_epoch_to_date(sdf_financials)\n",
    "print(f'sdf_all_income_stmt : {sdf_financials.count()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SÃ©lection des postes financier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "revenue = \"TotalRevenue\"\n",
    "cogs = \"CostOfRevenue\"\n",
    "opex = \"OperatingExpense\"\n",
    "da = \"ReconciledDepreciation\"\n",
    "interest = \"NetInterestIncome\"\n",
    "taxe = \"TaxProvision\"\n",
    "\n",
    "gross_margin = \"GrossProfit\"\n",
    "ebitda = \"EBITDA\"\n",
    "ebit = \"EBIT\"\n",
    "ebt = \"PretaxIncome\"\n",
    "net_income = \"NetIncome\"\n",
    "\n",
    "accounts_receivable = \"AccountsReceivable\"\n",
    "accounts_payable = \"AccountsPayable\"\n",
    "inventory = \"Inventory\"\n",
    "ppe = \"NetPPE\"\n",
    "other_asset = \"OtherNonCurrentAssets\" ## financial + other asset merged in yahoo\n",
    "non_current_liab = \"TotalNonCurrentLiabilitiesNetMinorityInterest\"\n",
    "current_liab = \"CurrentLiabilities\"\n",
    "intangible_asset = \"GoodwillAndOtherIntangibleAssets\" \n",
    "equity = \"TotalEquityGrossMinorityInterest\"\n",
    "cash = \"CashCashEquivalentsAndShortTermInvestments\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. explore forecasting methods for multiple ticker for sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_total_revenue = sdf_financials.filter(F.col('metric') == revenue)\n",
    "sdf_total_revenue = sdf_total_revenue.withColumn(\"year\", F.year('datetime'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_forecast_pandas(pdf: pd.DataFrame) -> pd.DataFrame:\n",
    "    # keep it simple: y = a + b * year\n",
    "    X = pdf[[\"year\"]]#.to_numpy()\n",
    "    y = pdf[\"value\"]#.to_numpy()\n",
    "\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X, y)\n",
    "\n",
    "    futur_years = [year for year in range(2025, 2031)]\n",
    "    X_futur = pd.DataFrame({'year' : futur_years})\n",
    "    y_pred = lr.predict(X_futur)\n",
    "\n",
    "    result = pd.DataFrame({\n",
    "        \"ticker\": pdf[\"ticker\"].iloc[0],\n",
    "        \"year\": futur_years,\n",
    "        \"metric\": revenue,\n",
    "        \"pred\": y_pred\n",
    "    })\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_sales_forecast = (\n",
    "    sdf_total_revenue.groupBy(\"ticker\")\n",
    "      .applyInPandas(fit_and_forecast_pandas, schema=\"ticker string, year int, metric string, pred double\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_sales_forecast = sdf_sales_forecast.\\\n",
    "    withColumn('datetime', make_date(col('year'), lit(\"12\"), lit(\"31\"))).\\\n",
    "    drop(\"year\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# building income statement - forecast for multiple tickers at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pct_metric_to_base(df, metric, base):\n",
    "    result = df.groupBy(col('ticker')).\\\n",
    "        agg(\n",
    "            (sum(when(col('metric') == metric, col('value')).otherwise(0)) / \n",
    "             sum(when(col('metric') == base, col('value')).otherwise(0))).alias(f'avg_{metric}_to_{base}_pct')\n",
    "        )\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_forecast_metrics(sales_forecast, df):\n",
    "    metrics = {cogs: revenue, opex: revenue, da: revenue, interest: revenue, taxe: ebt}\n",
    "    result_df = sales_forecast\n",
    "    for metric, base in metrics.items():\n",
    "        pct = compute_pct_metric_to_base(df, metric, base)\n",
    "        forecast = sales_forecast.join(pct, on=\"ticker\").select(\n",
    "            (col('pred')*col(f'avg_{metric}_to_{base}_pct')).alias('pred'),\n",
    "            lit(metric).alias('metric'),\n",
    "            col('datetime'),\n",
    "            col('ticker')\n",
    "            )\n",
    "        ### check taxe calculation, pct * ebt not sales.\n",
    "        result_df = result_df.unionByName(forecast)\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|ticker|count|\n",
      "+------+-----+\n",
      "|   NOV|   36|\n",
      "|  GOOG|   36|\n",
      "|   TGT|   36|\n",
      "|  PYPL|   36|\n",
      "|  AMZN|   36|\n",
      "|  NVDA|   36|\n",
      "|  SOPH|   36|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "compute_forecast_metrics(sdf_sales_forecast, sdf_financials).groupBy(col('ticker')).count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balance sheet analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = spark_df_all_balance_sheet.filter((col('ticker') == 'AMZN') & (col('datetime') == '2024-12-31'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "asset = temp.filter(col('metric').\\\n",
    "                    isin(accounts_receivable, inventory, cash, intangible_asset, other_asset, ppe)).\\\n",
    "                        agg(sum(col('value')).alias('total asset'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "payables = temp.filter(col('metric')==accounts_payable).agg(sum(col('value')).alias('payables'))\n",
    "liab = temp.filter(col('metric').isin(current_liab, non_current_liab)).agg(sum(col('value')).alias('liab'))\n",
    "other_liab = liab.crossJoin(payables).selectExpr(\"liab - payables as other_liab\")\n",
    "_equity = temp.filter(col('metric') == equity).agg(sum(col('value')).alias('equity'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## verifiaction balance sheet balances\n",
    "_equity.collect()[0][0] + other_liab.collect()[0][0] + payables.collect()[0][0] -\\\n",
    "      asset.collect()[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balance sheet forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DSO, DIO & DPO calculations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ticker: string, datetime: date, metric: string, value: double, metric: string, value: double]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## DSO = trades receivables / sales * 360\n",
    "## DPO = trades payable / cogs * 360\n",
    "## DIO = inventory / cogs * 360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = spark_df_all_income_stmt.filter(col('metric').isin(cogs, revenue)).\\\n",
    "    groupBy(\"ticker\", \"datetime\").pivot('metric').agg(F.first('value'))\n",
    "\n",
    "spark_df_all_balance_sheet = spark_df_all_balance_sheet.join(temp, on=['ticker', 'datetime'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# forecast balance sheet items"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5117293961394131,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "yahoo_api_exploration",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "yahoo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
